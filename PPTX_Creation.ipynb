{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKZBgGdKbTeoYKt6NoJIGp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrahmani/Python/blob/main/PPTX_Creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PPT Slide Creation"
      ],
      "metadata": {
        "id": "ZlmHKRAtKlVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-pptx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T0iEnPvKlu-",
        "outputId": "9c31ea64-7f2b-46ca-b6a1-6966ff62b80a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.12/dist-packages (from python-pptx) (11.3.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-pptx) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-pptx) (4.15.0)\n",
            "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.2.9 python-pptx-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuKke0pWKkFP",
        "outputId": "bb4ecd2e-cbf5-435d-8092-cf217fc6da45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Presentation saved as Large_Scale_Data_Visualisation.pptx\n"
          ]
        }
      ],
      "source": [
        "from pptx import Presentation\n",
        "from pptx.util import Inches, Pt\n",
        "from pptx.enum.text import PP_ALIGN\n",
        "from pptx.dml.color import RGBColor # Import RGBColor\n",
        "\n",
        "def create_presentation():\n",
        "    prs = Presentation()\n",
        "\n",
        "    # Slide content data\n",
        "    slides_data = [\n",
        "        {\n",
        "            \"title\": \"Introduction to Large-Scale Data Visualisation\",\n",
        "            \"content\": [\n",
        "                \"Large-scale data means working with very big datasets. These datasets may not fit easily into memory.\",\n",
        "                \"Visualising large data is harder because traditional tools may become slow. They may even crash.\",\n",
        "                \"This chapter teaches methods to handle such data safely. It helps students prepare for real business needs.\",\n",
        "                \"We focus on techniques that make visualisation faster. We also aim to reduce system load.\",\n",
        "                \"You will learn practical strategies used by data analysts and engineers used in industry projects.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Why Large-Scale Data Is Challenging\",\n",
        "            \"content\": [\n",
        "                \"Large data consumes a lot of memory. When memory is full, software becomes slow.\",\n",
        "                \"Processing time increases with dataset size. More data means more computation.\",\n",
        "                \"Visualisation tools may struggle to draw millions of points. Some tools freeze or stop responding.\",\n",
        "                \"Large data often needs cleaning before visualisation. Cleaning large data takes even more time.\",\n",
        "                \"Effective visualisation requires smart techniques. Without them, charts may be misleading or messy.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of a computer with a memory overload error message\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Common Sources of Large Datasets\",\n",
        "            \"content\": [\n",
        "                \"Large datasets come from web logs. Websites track millions of user activities.\",\n",
        "                \"Sensors generate large data every second. This includes IoT devices and industrial sensors.\",\n",
        "                \"Social media platforms produce huge text, image, and video data. Companies mine this data for insights.\",\n",
        "                \"Business operations such as banking create large transactional data. These records grow daily.\",\n",
        "                \"Scientific fields produce huge data through experiments. These include astronomy, weather, and medical scans.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of big data sources ecosystem\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Key Concepts in Large-Scale Data Visualisation\",\n",
        "            \"content\": [\n",
        "                \"Scalability means handling increasing data size without crashing. Visualisation tools must scale with data.\",\n",
        "                \"Efficiency means using resources wisely. It avoids wasting time or memory.\",\n",
        "                \"Abstraction hides unnecessary details. This makes charts simple and readable.\",\n",
        "                \"Data reduction decreases the size of data. It helps tools load data faster.\",\n",
        "                \"Progressive analytics shows results step by step. It helps users see partial results quickly.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Data Reduction: Introduction\",\n",
        "            \"content\": [\n",
        "                \"Data reduction reduces the amount of data we need to process. It makes analysis faster.\",\n",
        "                \"Too much detail can make charts confusing. Reduction keeps only important information.\",\n",
        "                \"Reduced data fits easily in memory. This improves performance.\",\n",
        "                \"Data reduction supports real-time visualisation. Users get quick responses.\",\n",
        "                \"It is one of the most important strategies for handling big data. Analysts use it daily.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Sampling Techniques\",\n",
        "            \"content\": [\n",
        "                \"Sampling picks a small part of the data. This smaller set represents the full dataset.\",\n",
        "                \"Random sampling selects data points randomly. It reduces bias.\",\n",
        "                \"Stratified sampling divides data into groups before sampling. It ensures each group is represented.\",\n",
        "                \"Systematic sampling picks every nth data point. It is simple and fast.\",\n",
        "                \"Sampling reduces processing time while keeping patterns visible. It is widely used in business dashboards.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of random vs stratified sampling comparison\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Aggregation Techniques\",\n",
        "            \"content\": [\n",
        "                \"Aggregation groups data into summaries. Examples include averages or totals.\",\n",
        "                \"It reduces the number of points displayed. This makes charts cleaner.\",\n",
        "                \"Time-series aggregation groups data by hour, day, or month. It helps highlight trends.\",\n",
        "                \"Spatial aggregation groups geographic points. It is used in maps and geodata.\",\n",
        "                \"Aggregation helps users focus on high-level patterns. It removes small-scale noise.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of data aggregation visualization process\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Binning Techniques\",\n",
        "            \"content\": [\n",
        "                \"Binning groups values into buckets. Each bucket shows a range of values.\",\n",
        "                \"Histograms are an example of binning. They show frequency distributions.\",\n",
        "                \"Binning simplifies large numeric data. It makes patterns easier to understand.\",\n",
        "                \"The number of bins affects chart readability. Too many bins cause clutter.\",\n",
        "                \"Binning is effective for millions of data points. It reduces complexity.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of histogram construction from raw data\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Clustering Techniques\",\n",
        "            \"content\": [\n",
        "                \"Clustering groups similar data points. Each group shares common features.\",\n",
        "                \"Algorithms such as k-means help reduce large data complexity. They summarise the dataset into clusters.\",\n",
        "                \"Clustering reduces visual clutter. It replaces millions of points with a few cluster centers.\",\n",
        "                \"Visualisations become easier to interpret. Users focus on patterns rather than noise.\",\n",
        "                \"Clustering is used in marketing, finance, and image analysis. It helps segment large data.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of k-means clustering scatter plot\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Dimensionality Reduction\",\n",
        "            \"content\": [\n",
        "                \"Dimensionality reduction removes unnecessary features. This reduces dataset size.\",\n",
        "                \"PCA (Principal Component Analysis) is a common technique. It identifies important dimensions.\",\n",
        "                \"t-SNE preserves local patterns. It is popular in deep learning.\",\n",
        "                \"UMAP is fast for very large datasets. It works well for visualising embeddings.\",\n",
        "                \"Reducing dimensions helps create clean 2D or 3D plots. It is useful for big, complex datasets.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of dimensionality reduction 3D to 2D projection\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Progressive Visualisation\",\n",
        "            \"content\": [\n",
        "                \"Progressive visualisation shows results in stages. It does not wait for full computation.\",\n",
        "                \"Users can begin exploring early results. This improves experience.\",\n",
        "                \"Systems add more detail over time. This makes visualisation feel responsive.\",\n",
        "                \"Progressive rendering works well for large datasets. It avoids long waiting times.\",\n",
        "                \"Many modern dashboards use progressive loading. It keeps users engaged.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of progressive image loading sequence\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Incremental Processing\",\n",
        "            \"content\": [\n",
        "                \"Incremental processing handles data in smaller chunks. It avoids loading everything at once.\",\n",
        "                \"Each chunk is processed and visualised separately. This reduces memory pressure.\",\n",
        "                \"It is useful for streaming data. Data arrives continuously in this case.\",\n",
        "                \"Incremental processing supports real-time dashboards. It updates charts instantly.\",\n",
        "                \"It is essential for large-scale systems such as social media analytics.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Streaming Visualisation\",\n",
        "            \"content\": [\n",
        "                \"Streaming visualisation handles data that arrives nonstop. Examples include financial markets.\",\n",
        "                \"It must update visuals instantly. Users expect real-time results.\",\n",
        "                \"Tools like Kafka, Spark Streaming, and Flink support streaming. They manage fast data flows.\",\n",
        "                \"Charts must be highly optimized. Too much detail slows down updates.\",\n",
        "                \"Streaming is used in IoT and cybersecurity. It helps detect real-time patterns.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of real-time data streaming architecture\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Tile-Based Rendering\",\n",
        "            \"content\": [\n",
        "                \"Tile-based rendering divides visuals into small parts called tiles. Only needed tiles are updated.\",\n",
        "                \"This reduces memory use. The computer draws fewer pixels.\",\n",
        "                \"It is used in map visualisation. Google Maps uses tiles.\",\n",
        "                \"Tiles load smoothly when zooming. It gives a better user experience.\",\n",
        "                \"It improves speed for very large geographic datasets. Tiles prevent lag.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of map tile rendering system\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Level of Detail (LOD) Techniques\",\n",
        "            \"content\": [\n",
        "                \"LOD shows different details depending on zoom level. More zoom means more detail.\",\n",
        "                \"This makes visualisation efficient. It avoids drawing too much at once.\",\n",
        "                \"Tools choose lower-detail data when zoomed out. This saves memory.\",\n",
        "                \"Higher detail appears only when needed. This keeps visuals clean.\",\n",
        "                \"LOD helps map, 3D models, and scientific visualisation. It is key for large data.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of level of detail 3D model comparison\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Parallel Processing\",\n",
        "            \"content\": [\n",
        "                \"Parallel processing uses multiple CPU cores. It processes data faster.\",\n",
        "                \"It splits tasks into smaller parts. Each part runs at the same time.\",\n",
        "                \"It is effective for large-scale visualisation. It handles heavy computation.\",\n",
        "                \"Software must be designed for parallelism. Not all programs support it.\",\n",
        "                \"Parallelism improves performance significantly. It is used in big data tools.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of parallel processing CPU architecture\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"GPU Acceleration\",\n",
        "            \"content\": [\n",
        "                \"GPUs are faster for drawing graphics. They handle many operations at once.\",\n",
        "                \"GPU-based tools like RAPIDS speed up visualisation. They use parallel computation.\",\n",
        "                \"GPUs handle millions of points smoothly. They avoid slowdowns.\",\n",
        "                \"This method is useful for scientific and AI visualisation. Large datasets require powerful tools.\",\n",
        "                \"GPU acceleration is widely adopted in modern dashboards. It enhances performance.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of CPU vs GPU architecture comparison\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Caching Strategies\",\n",
        "            \"content\": [\n",
        "                \"Caching stores computed results temporarily. This avoids repeating work.\",\n",
        "                \"It reduces loading time for large charts. Users see data faster.\",\n",
        "                \"Dashboards use caching for repeated queries. It saves time and resources.\",\n",
        "                \"Cache must be refreshed carefully. Old cache can show outdated information.\",\n",
        "                \"Caching improves overall performance. It is critical for real-time systems.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Indexing Methods\",\n",
        "            \"content\": [\n",
        "                \"Indexing organizes data for fast search. It is like a table of contents.\",\n",
        "                \"Indexes speed up large database queries. They find data quickly.\",\n",
        "                \"Visualisation tools use indexes to load relevant data. This avoids scanning everything.\",\n",
        "                \"Spatial indexes help with map data. They find nearby points efficiently.\",\n",
        "                \"Indexing is important for performance optimization. It supports smooth visualisation.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of database indexing structure\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Data Partitioning\",\n",
        "            \"content\": [\n",
        "                \"Partitioning divides data into smaller pieces. Each piece is easier to handle.\",\n",
        "                \"It helps distribute data across systems. This improves performance.\",\n",
        "                \"Partitions can be based on time, location, or category. Different rules apply.\",\n",
        "                \"Tools load only needed partitions. This speeds up visualisation.\",\n",
        "                \"Partitioning is used in big data platforms like Hadoop. It is essential for scalability.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of database partitioning diagram\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Database Optimization for Visualisation\",\n",
        "            \"content\": [\n",
        "                \"Databases must be optimized for fast queries. Slow queries impact visualisation.\",\n",
        "                \"Indexing, caching, and partitioning help speed up queries. They reduce workload.\",\n",
        "                \"Proper schema design improves performance. Well-designed tables load faster.\",\n",
        "                \"Column-based stores handle large analytical queries well. They compress data.\",\n",
        "                \"Database optimization ensures visualisation tools run efficiently. It supports interactive dashboards.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Handling Real-Time Data\",\n",
        "            \"content\": [\n",
        "                \"Real-time data arrives rapidly. Visualisation must keep up.\",\n",
        "                \"Streaming systems process real-time data. They ensure low latency.\",\n",
        "                \"Dashboards must be optimized. Too many updates slow down visuals.\",\n",
        "                \"Techniques like windowing summarise real-time streams. This reduces noise.\",\n",
        "                \"Real-time visualisation is used in finance, IoT, and security. It helps detect fast changes.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of a live, constantly updating dashboard\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Memory Management\",\n",
        "            \"content\": [\n",
        "                \"Efficient memory use is critical for large datasets. Small mistakes cause crashes.\",\n",
        "                \"Tools should avoid loading all data at once. Partial loading is safer.\",\n",
        "                \"Compression reduces memory size. It stores data more efficiently.\",\n",
        "                \"Removing unused objects prevents waste. Good memory hygiene is important.\",\n",
        "                \"Memory management supports smooth visualisation. It ensures stable performance.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Compression Techniques\",\n",
        "            \"content\": [\n",
        "                \"Compression reduces data size. It keeps important details but uses less space.\",\n",
        "                \"Lossless compression keeps all original data. It is good for sensitive applications.\",\n",
        "                \"Lossy compression removes small details. It creates smaller files.\",\n",
        "                \"Compression helps speed up data transfer. Smaller files load faster.\",\n",
        "                \"Good compression improves visualisation performance. It reduces system load.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Out-of-Core Processing\",\n",
        "            \"content\": [\n",
        "                \"Out-of-core processing handles data that does not fit in memory. It stores data on disk.\",\n",
        "                \"Tools process small chunks from the disk. This avoids memory overflow.\",\n",
        "                \"It is slower than in-memory processing. But it allows handling very large data.\",\n",
        "                \"Out-of-core methods help with gigabyte-scale datasets. They prevent crashes.\",\n",
        "                \"Many big data tools use this method. It supports scalable visualisation.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of out-of-core memory processing diagram\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Distributed Systems\",\n",
        "            \"content\": [\n",
        "                \"Distributed systems use multiple computers. They work together as one.\",\n",
        "                \"Large datasets are split across machines. Each machine processes a portion.\",\n",
        "                \"Distributed visualisation can scale to very large data. It avoids single-machine limits.\",\n",
        "                \"Tools like Hadoop and Spark support distributed work. They handle petabyte-scale data.\",\n",
        "                \"Distributed systems improve speed and capacity. They are common in industry analytics.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of distributed computing cluster architecture\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Cloud-Based Visualisation\",\n",
        "            \"content\": [\n",
        "                \"Cloud platforms provide scalable resources. They grow as needed.\",\n",
        "                \"Cloud tools handle large datasets easily. They offer strong computing power.\",\n",
        "                \"Cloud visualisation supports remote collaboration. Teams work from anywhere.\",\n",
        "                \"Services like AWS QuickSight and Google Data Studio support large-scale dashboards.\",\n",
        "                \"Cloud solutions are cost-effective for big data. Companies pay only for what they use.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Visual Abstraction Techniques\",\n",
        "            \"content\": [\n",
        "                \"Abstraction hides unnecessary details. It focuses on key information.\",\n",
        "                \"It helps simplify complex datasets. Users see clear patterns.\",\n",
        "                \"Techniques include summarisation and filtering. They remove less important points.\",\n",
        "                \"Abstract visuals load faster. They reduce clutter.\",\n",
        "                \"Abstraction is essential for large-scale visualisation. It improves readability.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Choosing the Right Visualisation Tool\",\n",
        "            \"content\": [\n",
        "                \"Tools differ in performance. Some are better for big data.\",\n",
        "                \"Tools like D3.js offer flexibility. But they may struggle with millions of points.\",\n",
        "                \"Tools like Datashader handle large data well. They use powerful rendering engines.\",\n",
        "                \"Business tools like Power BI and Tableau can process millions of rows. But performance depends on settings.\",\n",
        "                \"Choosing the right tool is important. It affects speed and results.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Techniques for Faster Rendering\",\n",
        "            \"content\": [\n",
        "                \"Reducing visual elements improves speed. Fewer points mean faster drawing.\",\n",
        "                \"Using vector graphics helps scaling. Vectors resize smoothly.\",\n",
        "                \"Using lightweight themes reduces rendering time. Heavy effects slow performance.\",\n",
        "                \"Removing unnecessary labels avoids clutter. It improves readability.\",\n",
        "                \"Efficient rendering makes charts feel responsive. It improves user satisfaction.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Handling Missing Data\",\n",
        "            \"content\": [\n",
        "                \"Missing data appears often in large datasets. It affects visual accuracy.\",\n",
        "                \"Techniques include removing, filling, or ignoring missing values. Each has pros and cons.\",\n",
        "                \"Removing rows is fast but may lose valuable information. It must be done carefully.\",\n",
        "                \"Filling missing values adds assumptions. It may change patterns.\",\n",
        "                \"Clear decisions on missing data improve visual reliability. Charts become more trustworthy.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of missing data imputation methods\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Avoiding Visual Overload\",\n",
        "            \"content\": [\n",
        "                \"Too much detail confuses users. This is called visual overload.\",\n",
        "                \"Overloaded charts are hard to read. Students may miss important insights.\",\n",
        "                \"Reducing elements improves clarity. It highlights main patterns.\",\n",
        "                \"Using fewer colours helps readability. Too many colours distract viewers.\",\n",
        "                \"Good visual design avoids cognitive overload. It supports better learning.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of data visualization clutter vs clean comparison\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Evaluating Visualisation Performance\",\n",
        "            \"content\": [\n",
        "                \"Performance must be tested. This ensures visualisations run smoothly.\",\n",
        "                \"Load time is an important measure. Fast loading increases usability.\",\n",
        "                \"Responsiveness tests check user interaction speed. Slow reactions cause frustration.\",\n",
        "                \"Memory usage is another measure. High memory use may cause crashes.\",\n",
        "                \"Performance evaluation improves system stability. It ensures good user experience.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Best Practices for Large-Scale Visualisation\",\n",
        "            \"content\": [\n",
        "                \"Always clean data first. Clean data speeds up processing.\",\n",
        "                \"Use reduction techniques when data is huge. It avoids overload.\",\n",
        "                \"Select the right visual type. Some charts are better for large data.\",\n",
        "                \"Test performance on different devices. Performance varies.\",\n",
        "                \"Keep visuals simple and clear. Simplicity improves understanding.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Case Study: Millions of Taxi Trips\",\n",
        "            \"content\": [\n",
        "                \"NYC Taxi dataset contains millions of rows. It is used for big data examples.\",\n",
        "                \"Without reduction, visualisation freezes tools. The data is too large.\",\n",
        "                \"Aggregation helps show trends by day or month. This reveals patterns.\",\n",
        "                \"Clustering highlights pickup hotspots. It simplifies geographic patterns.\",\n",
        "                \"Progressive loading allows early exploration. Users don't wait long.\"\n",
        "            ],\n",
        "            \"image_note\": \"Image of NYC taxi trip data heatmap\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Summary and Key Takeaways\",\n",
        "            \"content\": [\n",
        "                \"Large-scale visualisation needs special techniques. Normal methods may fail.\",\n",
        "                \"Data reduction is essential. It keeps charts clean and fast.\",\n",
        "                \"Performance optimization improves user experience. It reduces loading time.\",\n",
        "                \"Tools must be chosen carefully. Some tools handle big data better.\",\n",
        "                \"These methods prepare students for real-world analytics. They are important for future careers.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Activity: Data Reduction Game (Group)\",\n",
        "            \"content\": [\n",
        "                \"Each group receives a large printed dataset with many numbers.\",\n",
        "                \"Students must reduce the data using sampling. They choose the best sampling strategy.\",\n",
        "                \"Each group must explain why they chose that method. They must justify their reasoning.\",\n",
        "                \"Groups draw a small chart using the reduced data. They compare results with other groups.\",\n",
        "                \"Discussion reveals how reduction changes visual patterns.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Activity: Clustering Race (Physical + Mental)\",\n",
        "            \"content\": [\n",
        "                \"The class room becomes a clustering space. Each student represents a 'data point'.\",\n",
        "                \"Students move to form clusters based on a teacher-given rule (age, colour, shoe size).\",\n",
        "                \"Groups must decide how many clusters make sense. They must explain their choice.\",\n",
        "                \"The fastest correct clustering team wins. Speed and accuracy both matter.\",\n",
        "                \"Activity helps students understand clustering conceptually.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Activity: Progressive Visualisation Challenge\",\n",
        "            \"content\": [\n",
        "                \"Groups receive a dataset printed in chunks. Each chunk arrives every 30 seconds.\",\n",
        "                \"Students must draw a visualisation that improves as chunks arrive. They update charts progressively.\",\n",
        "                \"Groups discuss how early visuals differ from final visuals.\",\n",
        "                \"They must write what insights appeared early and what appeared later.\",\n",
        "                \"The best explanation wins. Clarity and understanding matter most.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Activity: Performance Optimization Puzzle\",\n",
        "            \"content\": [\n",
        "                \"Each group receives a list of performance problems (slow load times, memory issues).\",\n",
        "                \"Students must match each problem with the correct solution (caching, sampling, indexing).\",\n",
        "                \"They work as a team to solve the puzzle quickly.\",\n",
        "                \"Groups present their solutions. They explain their reasoning.\",\n",
        "                \"Teacher clarifies correct matches.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Activity: Visual Overload Cleanup Game\",\n",
        "            \"content\": [\n",
        "                \"Teacher shows a messy and overloaded chart. It contains too many colours and labels.\",\n",
        "                \"Groups must redesign the chart on paper. They remove unnecessary elements.\",\n",
        "                \"Each group explains why they removed certain items. They justify design choices.\",\n",
        "                \"Groups compare their improved charts.\",\n",
        "                \"Best redesign wins. Focus is on clarity and simplicity.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Lab Activities (Home Practice)\",\n",
        "            \"content\": [\n",
        "                \"Lab 1: Load a large dataset and apply random and stratified sampling. Compare the results in charts.\",\n",
        "                \"Lab 2: Group data by time or category. Create bar charts and line charts showing aggregated values.\",\n",
        "                \"Lab 3: Use k-means on a large dataset. Visualise cluster centers and discuss findings.\",\n",
        "                \"Lab 4: Simulate chunk-based loading. Display data step-by-step using your preferred tool.\",\n",
        "                \"Lab 5: Create a heavy chart and improve its performance using indexing, caching, or reduction.\"\n",
        "            ],\n",
        "            \"image_note\": None\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Create slides\n",
        "    for slide_data in slides_data:\n",
        "        # Use Layout 1 (Title and Content)\n",
        "        slide_layout = prs.slide_layouts[1]\n",
        "        slide = prs.slides.add_slide(slide_layout)\n",
        "\n",
        "        # Set Title\n",
        "        title = slide.shapes.title\n",
        "        title.text = slide_data['title']\n",
        "\n",
        "        # Set Body Content (Bullet Points)\n",
        "        body_shape = slide.shapes.placeholders[1]\n",
        "        tf = body_shape.text_frame\n",
        "        tf.text = slide_data['content'][0] # First bullet point\n",
        "\n",
        "        for bullet in slide_data['content'][1:]:\n",
        "            p = tf.add_paragraph()\n",
        "            p.text = bullet\n",
        "            p.level = 0\n",
        "\n",
        "        # Add Image Placeholder if note exists\n",
        "        if slide_data['image_note']:\n",
        "            # Add a box at the bottom or side\n",
        "            left = Inches(5)\n",
        "            top = Inches(2)\n",
        "            width = Inches(4)\n",
        "            height = Inches(3)\n",
        "            textbox = slide.shapes.add_textbox(left, top, width, height)\n",
        "            textbox.text = f\"PLACEHOLDER:\\n{slide_data['image_note']}\"\n",
        "            p = textbox.text_frame.paragraphs[0]\n",
        "            p.alignment = PP_ALIGN.CENTER\n",
        "\n",
        "            # Add a border to the placeholder so it's visible\n",
        "            line = textbox.line\n",
        "            line.color.rgb = RGBColor(0, 0, 0) # Set border color to black\n",
        "            line.width = Pt(2)\n",
        "\n",
        "    # Save\n",
        "    prs.save('Large_Scale_Data_Visualisation.pptx')\n",
        "    print(\"Presentation saved as Large_Scale_Data_Visualisation.pptx\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_presentation()\n"
      ]
    }
  ]
}